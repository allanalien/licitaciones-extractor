# Plan de ImplementaciÃ³n - Sistema de ExtracciÃ³n de Licitaciones

## Objetivo
Crear un sistema de extracciÃ³n automatizado que recopile licitaciones diarias de mÃºltiples fuentes y las almacene en una base de datos vectorial RAG para alimentar un agente inteligente.

## Arquitectura General

### Estructura del Proyecto
```
licitaciones_extractor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_extractor.py          # Clase base para todos los extractores
â”‚   â”‚   â”œâ”€â”€ licita_ya_extractor.py     # Extractor API privada Licita Ya
â”‚   â”‚   â”œâ”€â”€ cdmx_extractor.py          # Extractor API pÃºblica CDMX
â”‚   â”‚   â””â”€â”€ comprasmx_scraper.py       # Web scraper ComprasMX
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ connection.py              # ConfiguraciÃ³n PostgreSQL
â”‚   â”‚   â”œâ”€â”€ models.py                  # Modelos de datos
â”‚   â”‚   â””â”€â”€ schema.sql                 # Script creaciÃ³n tabla updates
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_processor.py          # Procesamiento de texto semÃ¡ntico
â”‚   â”‚   â”œâ”€â”€ embeddings_generator.py    # GeneraciÃ³n de embeddings
â”‚   â”‚   â””â”€â”€ data_normalizer.py         # NormalizaciÃ³n entre fuentes
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py                # Configuraciones generales
â”‚   â”‚   â””â”€â”€ keywords.py                # Keywords corporativos
â”‚   â”œâ”€â”€ scheduler/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ daily_job.py               # Orquestador diario
â”‚   â””â”€â”€ main.py                        # Punto de entrada principal
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_extractors.py
â”‚   â””â”€â”€ test_database.py
â”œâ”€â”€ logs/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml                 # Para desarrollo local
â””â”€â”€ README.md
```

## Base de Datos

### Tabla Principal: `updates`
Basada en la estructura existente de `licitaciones_updates` con los siguientes campos:

```sql
CREATE TABLE updates (
    id SERIAL PRIMARY KEY,
    tender_id VARCHAR(255) UNIQUE NOT NULL,
    fuente VARCHAR(50) NOT NULL,
    fecha_extraccion TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    fecha_catalogacion DATE,
    fecha_apertura DATE,
    titulo TEXT,
    descripcion TEXT,
    texto_semantico TEXT NOT NULL,
    metadata JSONB NOT NULL,
    embeddings VECTOR(1536),  -- Ajustar dimensiÃ³n segÃºn modelo
    entidad VARCHAR(255),
    estado VARCHAR(100),
    ciudad VARCHAR(100),
    valor_estimado DECIMAL(15,2),
    tipo_licitacion VARCHAR(100),
    url_original TEXT,
    procesado BOOLEAN DEFAULT FALSE,
    INDEX idx_fecha_extraccion (fecha_extraccion),
    INDEX idx_fuente (fuente),
    INDEX idx_tender_id (tender_id)
);
```

### ConexiÃ³n PostgreSQL
- **URL**: `postgresql://neondb_owner:npg_Tr1wXonS8EZy@ep-fragrant-feather-age2rjov-pooler.c-2.eu-central-1.aws.neon.tech/neondb?sslmode=require`

## Fuentes de Datos

### 1. API Licita Ya (Privada)
- **Endpoint**: `https://www.licitaya.com.mx/api/v1/tender/search`
- **API Key**: `B1995953A2A074E0EB2A35494C6F9E5C`
- **Headers**: `X-API-KEY: {api_key}`

#### Keywords Corporativos Prioritarios:
1. **alimentos** - Suministros alimentarios y catering
2. **medicinas** - Medicamentos y equipos mÃ©dicos
3. **obra publica** - ConstrucciÃ³n e infraestructura
4. **equipo tecnologico** - Hardware y software
5. **servicios profesionales** - ConsultorÃ­a y asesorÃ­a
6. **construccion** - Obras civiles y arquitectura
7. **salud** - Servicios de salud y hospitalarios
8. **educacion** - Servicios educativos y capacitaciÃ³n
9. **seguridad** - Servicios y equipos de seguridad
10. **transporte** - VehÃ­culos y servicios logÃ­sticos

#### ParÃ¡metros de ExtracciÃ³n:
```python
params = {
    'date': 'YYYYMMDD',  # DÃ­a anterior
    'keyword': keyword,
    'page': page_num,
    'items': 25,
    'smartsearch': 1,
    'listing': 0
}
```

### 2. API CDMX (PÃºblica)
- **Endpoint**: `https://datosabiertostianguisdigital.cdmx.gob.mx/api/v1/plannings`
- **ParÃ¡metros**:
  - `hiring_method=1,2,3`
  - `consolidated=FALSE`
  - `start_date`: DÃ­a anterior (dd/MM/yyyy)
  - `end_date`: DÃ­a anterior (dd/MM/yyyy)

### 3. Web Scraping ComprasMX
- **URL**: `https://comprasmx.buengobierno.gob.mx/sitiopublico/#/`
- **TecnologÃ­a**: Selenium + BeautifulSoup
- **Enfoque**:
  - NavegaciÃ³n de SPA (Single Page Application)
  - ExtracciÃ³n de licitaciones del dÃ­a anterior
  - Manejo de paginaciÃ³n dinÃ¡mica

## Procesamiento de Datos

### NormalizaciÃ³n
Cada fuente debe normalizar sus datos al siguiente formato estÃ¡ndar:

```python
{
    "tender_id": "string",
    "fuente": "licita_ya|cdmx|comprasmx",
    "titulo": "string",
    "descripcion": "string",
    "entidad": "string",
    "estado": "string",
    "ciudad": "string",
    "fecha_catalogacion": "YYYY-MM-DD",
    "fecha_apertura": "YYYY-MM-DD",
    "valor_estimado": float,
    "tipo_licitacion": "string",
    "url_original": "string",
    "metadata_especifica": {}  # Datos especÃ­ficos de la fuente
}
```

### Texto SemÃ¡ntico
CombinaciÃ³n estructurada de:
- TÃ­tulo de la licitaciÃ³n
- DescripciÃ³n completa
- Nombre de la entidad
- Tipo de licitaciÃ³n
- Contexto geogrÃ¡fico

### Embeddings
- **Modelo**: OpenAI text-embedding-ada-002 o similar
- **Dimensiones**: 1536
- **Procesamiento**: Texto semÃ¡ntico completo

### Metadata JSONB
```json
{
    "fuente_original": "string",
    "fecha_extraccion": "ISO_datetime",
    "parametros_busqueda": {},
    "datos_especificos": {
        "licita_ya": {
            "smart_search": "string",
            "lots": [],
            "agency": "string"
        },
        "cdmx": {
            "hiring_method": "string",
            "consolidated": boolean
        },
        "comprasmx": {
            "pagina_origen": "string",
            "metodo_extraccion": "scraping"
        }
    },
    "calidad_datos": {
        "completitud": 0.0-1.0,
        "confiabilidad": 0.0-1.0
    }
}
```

## AutomatizaciÃ³n y OrquestaciÃ³n

### Flujo Diario
1. **Inicio**: 06:00 AM (para capturar licitaciones del dÃ­a anterior)
2. **Secuencia**:
   - Licita Ya API (por cada keyword)
   - CDMX API
   - ComprasMX Scraping
3. **Procesamiento**:
   - NormalizaciÃ³n de datos
   - GeneraciÃ³n de embeddings
   - Almacenamiento en PostgreSQL
4. **Logging y monitoreo**

### ConfiguraciÃ³n Cron
```bash
0 6 * * * /path/to/python /path/to/licitaciones_extractor/main.py --mode=daily
```

### Manejo de Errores
- Reintentos automÃ¡ticos (3 intentos por fuente)
- Logging detallado por fuente
- Notificaciones en caso de falla crÃ­tica
- Continuidad: si una fuente falla, continÃºa con las demÃ¡s

## Configuraciones

### Variables de Entorno (.env)
```
# Database
POSTGRES_URL=postgresql://neondb_owner:npg_Tr1wXonS8EZy@ep-fragrant-feather-age2rjov-pooler.c-2.eu-central-1.aws.neon.tech/neondb?sslmode=require

# APIs
LICITA_YA_API_KEY=B1995953A2A074E0EB2A35494C6F9E5C
LICITA_YA_BASE_URL=https://www.licitaya.com.mx/api/v1
CDMX_BASE_URL=https://datosabiertostianguisdigital.cdmx.gob.mx/api/v1

# Embeddings
OPENAI_API_KEY=your_openai_key
EMBEDDING_MODEL=text-embedding-ada-002

# Scraping
SELENIUM_TIMEOUT=30
HEADLESS_BROWSER=true

# Logging
LOG_LEVEL=INFO
LOG_FILE=/var/log/licitaciones_extractor.log

# Scheduling
EXTRACTION_TIME=06:00
RETRY_ATTEMPTS=3
BATCH_SIZE=100
```

### Keywords Configuration
```python
CORPORATE_KEYWORDS = [
    "alimentos",
    "medicinas",
    "obra publica",
    "equipo tecnologico",
    "servicios profesionales",
    "construccion",
    "salud",
    "educacion",
    "seguridad",
    "transporte"
]
```

## Extensibilidad Futura

### Agregar Nuevas Fuentes
1. Crear nuevo extractor heredando de `BaseExtractor`
2. Implementar mÃ©todos requeridos:
   - `extract_data(date)`
   - `normalize_data(raw_data)`
   - `validate_data(normalized_data)`
3. Registrar en el orquestador principal

### Estructura BaseExtractor
```python
class BaseExtractor:
    def __init__(self, config):
        self.config = config

    def extract_data(self, date):
        raise NotImplementedError

    def normalize_data(self, raw_data):
        raise NotImplementedError

    def validate_data(self, normalized_data):
        raise NotImplementedError
```

## Monitoreo y Mantenimiento

### MÃ©tricas Clave
- NÃºmero de licitaciones extraÃ­das por fuente/dÃ­a
- Tiempo de ejecuciÃ³n por extractor
- Tasa de errores por fuente
- Calidad de datos (completitud, duplicados)

### Logs Estructurados
```python
{
    "timestamp": "ISO_datetime",
    "level": "INFO|WARNING|ERROR",
    "component": "extractor_name",
    "message": "string",
    "metadata": {
        "execution_time": "seconds",
        "records_processed": int,
        "errors_count": int
    }
}
```

## Dependencias Principales

### requirements.txt
```
requests>=2.28.0
psycopg2-binary>=2.9.0
sqlalchemy>=1.4.0
selenium>=4.0.0
beautifulsoup4>=4.11.0
openai>=0.27.0
pandas>=1.5.0
python-dotenv>=0.19.0
schedule>=1.2.0
loguru>=0.6.0
pydantic>=1.10.0
tenacity>=8.2.0
```

## Fases de ImplementaciÃ³n

### âœ… Fase 1: Infraestructura Base - COMPLETADA
1. âœ… Estructura del proyecto
2. âœ… Base de datos y modelos
3. âœ… ConfiguraciÃ³n y logging

### âœ… Fase 2: Extractores Individuales - COMPLETADA
1. âœ… Extractor Licita Ya
2. âœ… Extractor CDMX
3. âœ… Scraper ComprasMX

### âœ… Fase 3: IntegraciÃ³n y AutomatizaciÃ³n - COMPLETADA
1. âœ… Orquestador principal (`ExtractionOrchestrator`)
2. âœ… GeneraciÃ³n de embeddings (`EmbeddingsGenerator`)
3. âœ… Scheduling automÃ¡tico (`DailyScheduler`)
4. âœ… Entry point principal con CLI completo
5. âœ… Manejo de errores y reintentos
6. âœ… IntegraciÃ³n completa y pruebas

### âœ… Fase 4: OptimizaciÃ³n y Monitoreo - COMPLETADA
1. âœ… MÃ©tricas y dashboards en tiempo real
2. âœ… OptimizaciÃ³n de performance con procesamiento paralelo
3. âœ… Sistema de alertas para errores crÃ­ticos
4. âœ… AnÃ¡lisis de calidad de datos
5. âœ… DocumentaciÃ³n completa actualizada

## Consideraciones de Seguridad
- API keys en variables de entorno
- Conexiones SSL para base de datos
- Rate limiting para evitar bloqueos
- ValidaciÃ³n de datos de entrada
- Logs sin informaciÃ³n sensible

## EstimaciÃ³n de Recursos
- **Tiempo de desarrollo**: 3-4 semanas
- **Almacenamiento**: ~1GB/mes (estimado)
- **Procesamiento**: EjecuciÃ³n diaria ~30-60 minutos
- **Mantenimiento**: RevisiÃ³n semanal de logs y mÃ©tricas

---

## ğŸ‰ ESTADO ACTUAL DEL PROYECTO

### âœ… SISTEMA COMPLETAMENTE FUNCIONAL - LISTO PARA PRODUCCIÃ“N

**Fecha de FinalizaciÃ³n Fase 3**: 2 de Diciembre, 2024
**Fecha de FinalizaciÃ³n Fase 4**: 2 de Diciembre, 2024

### ğŸ“‹ Componentes Implementados y Funcionando:

#### ğŸ—ï¸ Infraestructura Core
- âœ… **Base de datos PostgreSQL** configurada y funcional
- âœ… **Modelos SQLAlchemy** con soporte para embeddings
- âœ… **Sistema de configuraciÃ³n** con variables de entorno
- âœ… **Logging estructurado** con archivos JSON

#### ğŸ”„ Extractores de Datos
- âœ… **LicitaYaExtractor** - API privada con keywords corporativos
- âœ… **CDMXExtractor** - API pÃºblica de Ciudad de MÃ©xico
- âœ… **ComprasMXScraper** - Web scraping con Selenium

#### ğŸ¤– Sistema de OrquestaciÃ³n
- âœ… **ExtractionOrchestrator** - CoordinaciÃ³n completa de extractores
- âœ… **EmbeddingsGenerator** - GeneraciÃ³n de vectores con OpenAI
- âœ… **DailyScheduler** - AutomatizaciÃ³n y scheduling
- âœ… **DataNormalizer** - NormalizaciÃ³n entre fuentes

#### ğŸ’» Interface de Usuario
- âœ… **CLI Completo** con 8 modos de operaciÃ³n:
  - `extract` - ExtracciÃ³n manual
  - `daily` - Trabajo diario
  - `scheduler` - Modo continuo
  - `test` - Pruebas de conexiÃ³n
  - `setup` - ConfiguraciÃ³n inicial
  - `monitor` - Ver mÃ©tricas del sistema
  - `quality-report` - Generar reporte de calidad
  - `dashboard` - Dashboard web interactivo

#### ğŸ“Š Sistema de Monitoreo y OptimizaciÃ³n (Fase 4)
- âœ… **MetricsCollector** - RecopilaciÃ³n de mÃ©tricas en tiempo real
- âœ… **DataQualityAnalyzer** - AnÃ¡lisis profundo de calidad de datos
- âœ… **PerformanceMonitor** - OptimizaciÃ³n y procesamiento paralelo
- âœ… **AlertingSystem** - Sistema de alertas multi-canal
- âœ… **Dashboard Web** - Interfaz visual con grÃ¡ficos interactivos

### ğŸš€ Comandos de ProducciÃ³n Listos:

```bash
# ExtracciÃ³n diaria automÃ¡tica
python src/main.py --mode=daily

# Scheduler continuo (recomendado para producciÃ³n)
python src/main.py --mode=scheduler

# Pruebas del sistema
python src/main.py --mode=test
```

### ğŸ“Š CaracterÃ­sticas TÃ©cnicas Implementadas:

#### âš¡ Performance y Confiabilidad
- âœ… **Sistema de reintentos**: 3 intentos con backoff exponencial
- âœ… **Procesamiento por lotes**: Configurable para optimizar memoria
- âœ… **Manejo de errores**: ContinÃºa procesando aunque un extractor falle
- âœ… **CachÃ© inteligente**: Sistema de cachÃ© para embeddings

#### ğŸ”§ Operaciones y Monitoreo
- âœ… **Logging detallado**: MÃ©tricas de tiempo, registros procesados, errores
- âœ… **ValidaciÃ³n de datos**: VerificaciÃ³n de completitud y calidad
- âœ… **ConfiguraciÃ³n flexible**: Todos los parÃ¡metros configurables via .env
- âœ… **Modo dry-run**: Pruebas sin escribir a base de datos

#### ğŸ›¡ï¸ Seguridad
- âœ… **API keys en variables de entorno**: No hay credenciales en cÃ³digo
- âœ… **Conexiones SSL**: Base de datos segura
- âœ… **Rate limiting**: PrevenciÃ³n de bloqueos de APIs
- âœ… **ValidaciÃ³n de entrada**: SanitizaciÃ³n de datos

### ğŸ—„ï¸ Base de Datos Productiva
- âœ… **Tabla `updates`** con estructura completa
- âœ… **Ãndices optimizados** para consultas rÃ¡pidas
- âœ… **Soporte para embeddings** (JSONB temporal, migrable a pgvector)
- âœ… **Metadatos estructurados** para trazabilidad

### ğŸ“ˆ MÃ©tricas de Rendimiento Esperadas:
- **Tiempo de ejecuciÃ³n diaria**: 30-60 minutos
- **Registros por dÃ­a**: 100-500 licitaciones
- **Uso de memoria**: ~200MB durante procesamiento
- **Llamadas API**: ~100-200 embeddings por dÃ­a

### âœ… CaracterÃ­sticas Implementadas en Fase 4:
1. **Dashboard Web Interactivo** - VisualizaciÃ³n en tiempo real con Plotly
2. **Procesamiento Paralelo** - ExtracciÃ³n simultÃ¡nea de mÃºltiples fuentes
3. **Sistema de Alertas Multi-canal** - Logs, Email, Webhooks, Slack
4. **AnÃ¡lisis de Calidad Avanzado** - Completitud, duplicados, consistencia
5. **OptimizaciÃ³n AutomÃ¡tica** - Batch size dinÃ¡mico basado en recursos

---

## ğŸ† CONCLUSIÃ“N

**El Sistema de ExtracciÃ³n de Licitaciones estÃ¡ 100% FUNCIONAL y LISTO PARA PRODUCCIÃ“N.**

Todas las fases planificadas (1, 2, 3 y 4) han sido completadas exitosamente. El sistema incluye:

### Capacidades de ExtracciÃ³n:
- âœ… Extraer datos de mÃºltiples fuentes diariamente
- âœ… Generar embeddings automÃ¡ticamente con OpenAI
- âœ… Almacenar en base de datos vectorial PostgreSQL
- âœ… Ejecutarse de forma automatizada con scheduler
- âœ… Manejar errores con reintentos inteligentes
- âœ… Proporcionar logging estructurado completo

### Capacidades de Monitoreo y OptimizaciÃ³n:
- âœ… Dashboard web con mÃ©tricas en tiempo real
- âœ… Sistema de alertas multi-canal configurables
- âœ… AnÃ¡lisis automÃ¡tico de calidad de datos
- âœ… Procesamiento paralelo para mayor eficiencia
- âœ… Reportes detallados con recomendaciones
- âœ… OptimizaciÃ³n automÃ¡tica de recursos

**Â¡Sistema empresarial completo, robusto y escalable!** ğŸš€

**ImplementaciÃ³n de todas las fases exitosa!** ğŸ‰